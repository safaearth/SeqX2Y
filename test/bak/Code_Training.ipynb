{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPTDYSBHKs/llR2Vc2D59ZV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m0OteX0ojrqH"},"outputs":[],"source":["!pip install SimpleITK\n","!pip install pydicom"]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import torchvision\n","import sys\n","import numpy as np\n","import os\n","import csv\n","import SimpleITK as sitk\n","sys.path.append('/content/drive/MyDrive/PROJECT/RMSim_SeqX2Y/SeqX2Y-main')"],"metadata":{"id":"wvOablD2kDoK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Dataset Dataloader**"],"metadata":{"id":"re1BbTl2kSaL"}},{"cell_type":"code","source":["# CTデータセットclassを作成（CTDataset）：\n","class CTDataset(Dataset):\n","    # def __init__(self, file_paths, targets, transform=None):\n","    def __init__(self, file_paths, transform=None):\n","        self.file_paths = file_paths\n","        # self.targets = targets\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","    # def __getitem__(self):\n","        file_path = self.file_paths[idx]\n","\n","        # CT画像データの読み込む\n","        image = sitk.ReadImage(file_path)\n","        image_array = sitk.GetArrayFromImage(image)\n","\n","        # 画像データの前処理\n","        if self.transform:\n","            image_array = self.transform(image_array)\n","\n","        # 目標idを取得\n","        # target = self.targets[idx]\n","\n","        # 画像データとタグを返す\n","        # return image_array, target\n","        return image_array"],"metadata":{"id":"8Gjm66hGkP4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing the transform for the dataset\n","transform = torchvision.transforms.Compose([\n","\ttorchvision.transforms.ToTensor(),  # 画像をテンソルに変換\n","\ttorchvision.transforms.Normalize((0.5), (0.5))  # 画像データを正規化\n","])"],"metadata":{"id":"UFswfRX6kV99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# フォルダパスの定義、datasetとdataloaderのインスタンス化\n","folder_path = \"/content/drive/MyDrive/DataSet/4DCT_dicom/T00\"\n","file_paths = []\n","\n","# フォルダ内のファイルをトラバースしてfile_pathに保存\n","for file_name in sorted(os.listdir(folder_path)):\n","    file_path = os.path.join(folder_path, file_name)\n","    file_paths.append(file_path)\n","\n","# datasetのインスタンス化\n","dataset = CTDataset(file_paths)\n","\n","# dataloaderのインスタンス化\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"],"metadata":{"id":"JZQi4MNskYZV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **seq2seq_4DCT_voxelmorph**"],"metadata":{"id":"oJq2NUeHkdy_"}},{"cell_type":"code","source":["# seq2seq_4DCT_voxelmorph.py\n","import torch\n","import torch.nn as nn\n","\n","from models.ConvLSTMCell3d import ConvLSTMCell\n","from layers import SpatialTransformer\n","from models.unet_utils import *\n","\n","class EncoderDecoderConvLSTM(nn.Module):\n","    def __init__(self, nf, in_chan, size1, size2, size3):\n","        super(EncoderDecoderConvLSTM, self).__init__()\n","\n","        \"\"\" ARCHITECTURE\n","\n","        # Encoder (ConvLSTM)\n","        # Encoder Vector (final hidden state of encoder)\n","        # Decoder (ConvLSTM) - takes Encoder Vector as input\n","        # Decoder (3D CNN) - produces regression predictions for our model\n","\n","        \"\"\"\n","        # BxCx1xDxWxH\n","\n","        self.encoder1_conv = nn.Conv3d(in_channels=in_chan,\n","                        out_channels=nf,\n","                        kernel_size=(3, 3, 3),\n","                        padding=(1, 1, 1))\n","\n","        self.down1 = nn.MaxPool3d(kernel_size=2, stride=2)\n","\n","        self.ConvLSTM3d1 = ConvLSTMCell(input_dim=nf,\n","                        hidden_dim=nf,\n","                        kernel_size=(3,3,3),\n","                        bias=True)\n","        self.ConvLSTM3d2 = ConvLSTMCell(input_dim=nf,\n","                        hidden_dim=nf,\n","                        kernel_size=(3, 3, 3),\n","                        bias=True)\n","        self.ConvLSTM3d3 = ConvLSTMCell(input_dim=nf,\n","                        hidden_dim=nf,\n","                        kernel_size=(3, 3, 3),\n","                        bias=True)\n","        self.ConvLSTM3d4 = ConvLSTMCell(input_dim=nf,\n","                        hidden_dim=nf,\n","                        kernel_size=(3, 3, 3),\n","                        bias=True)\n","\n","        self.up1 = nn.Upsample(scale_factor=2, mode='trilinear')\n","\n","        self.out = ConvOut(nf)\n","\n","        self.transformer = SpatialTransformer((size1, size2, size3))\n","\n","\n","    def autoencoder(self, x, seq_len, rpm_x, rpm_y, future_step, h_t4, c_t4, h_t5, c_t5, h_t6, c_t6, h_t7, c_t7):\n","        latent = []\n","        out = []\n","        # encoder\n","        e1 = []\n","        e2 = []\n","        e3 = []\n","\n","        for t in range(seq_len):\n","            # print(rpm_x.shape, rpm_y.shape)\n","            h_t1 = self.encoder1_conv(x[:,t,...])\n","            down1 = self.down1(h_t1)\n","\n","            h_t4, c_t4 = self.ConvLSTM3d1(input_tensor=down1,\n","                                   cur_state=[h_t4,c_t4])\n","            h_t5, c_t5 = self.ConvLSTM3d2(input_tensor = h_t4,\n","                                   cur_state = [h_t5,c_t5])\n","            # h_t5 = torch.mul(h_t5,torch.squeeze(rpm_x[0,1]))\n","            h_t5 = torch.mul(h_t5,torch.squeeze(rpm_x[0,t-1]))\n","            # torch.squeeze(rpm_x[0, t-1])：这部分代码先使用 torch.squeeze() 函数将 rpm_x 张量中的大小为1的维度压缩（去掉），\n","            # 然后通过索引 [0, t-1] 获取 rpm_x 张量的特定元素。其中，t 是一个整数变量，表示一个时间步的索引。\n","            # torch.squeeze(rpm_x[0,t-1])：Remove the \"t-1\" position size 1, if not 1 else don't remove.\n","            # simple multiplication between rpm and feature\n","\n","            encoder_vector = h_t5\n","\n","\n","        for t in range(future_step):\n","\n","            h_t6, c_t6 = self.ConvLSTM3d3(input_tensor=encoder_vector,\n","                                   cur_state=[h_t6, c_t6])\n","            h_t7, c_t7 = self.ConvLSTM3d4(input_tensor=h_t6,\n","                                   cur_state=[h_t7, c_t7])\n","            h_t7 = torch.mul(h_t7, torch.squeeze(rpm_y[0,t]))\n","            # Simple multiplication between rpm and later phase features\n","            encoder_vector = h_t7\n","            latent += [h_t7]\n","\n","        latent = torch.stack(latent,1)\n","        latent = latent.permute(0,2,1,3,4,5)\n","        timestep = latent.shape[2]\n","\n","        output_img = []\n","        output_dvf = []\n","        # spatial transformer = transformer\n","        for i in range(timestep):\n","            output_ts = self.up1(latent[:,:,i,...])\n","            dvf = self.out(output_ts)\n","            warped_img = self.transformer(x[:,0,...],dvf)\n","            output_img += [warped_img]\n","            output_dvf += [dvf]\n","\n","        output_img = torch.stack(output_img,1)\n","        output_dvf = torch.stack(output_dvf,1)\n","        output_img = output_img.permute(0,2,1,3,4,5)\n","        output_dvf = output_dvf.permute(0,2,1,3,4,5)\n","\n","        return output_img, output_dvf\n","\n","\n","    def forward(self, x, rpm_x, rpm_y, future_seq=0, hidden_state=None):\n","\n","        \"\"\"\n","        Parameters\n","        ----------\n","        input_tensor:\n","            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, channel, height, width\n","        \"\"\"\n","\n","        # find size of different input dimensions\n","        b, seq_len, _, d, h, w = x.size()\n","\n","        # initialize hidden states\n","        # 当使用//运算符进行整数除法时，结果将会是一个整数，向下取整到最接近的整数值。这与普通的除法运算符/不同，后者执行的是浮点数除法，结果可以包含小数部分\n","        h_t4, c_t4 = self.ConvLSTM3d1.init_hidden(batch_size=b, image_size=(int(d // 2), int(h // 2), int(w // 2)))\n","        h_t5, c_t5 = self.ConvLSTM3d2.init_hidden(batch_size=b, image_size=(int(d // 2), int(h // 2), int(w // 2)))\n","        h_t6, c_t6 = self.ConvLSTM3d3.init_hidden(batch_size=b, image_size=(int(d // 2), int(h // 2), int(w // 2)))\n","        h_t7, c_t7 = self.ConvLSTM3d4.init_hidden(batch_size=b, image_size=(int(d // 2), int(h // 2), int(w // 2)))\n","\n","        # autoencoder forward\n","        # outputs = self.autoencoder(x, seq_len, future_seq, h_t1, c_t1, h_t2, c_t2, h_t3, c_t3, m_t3, h_t4, c_t4, m_t4,\n","        # h_t5, c_t5, m_t5, h_t6, c_t6, m_t6, h_t7, c_t7, h_t8, c_t8)\n","        outputs = self.autoencoder(x, seq_len, rpm_x, rpm_y, future_seq, h_t4, c_t4, h_t5, c_t5, h_t6, c_t6, h_t7, c_t7)\n","\n","        return outputs\n","\n","# Instantiating the model and hyperparameters\n","model = EncoderDecoderConvLSTM(nf=96, in_chan=1, size1=128, size2=128, size3=128)\n","# model = ConvLSTMCell(input_dim=1, hidden_dim=96, kernel_size=(3,3), bias=False)"],"metadata":{"id":"K0ccAixpkcZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Loss function**"],"metadata":{"id":"Q_y5OtBplmWH"}},{"cell_type":"code","source":["# MSE lossを使う\n","criterion = torch.nn.MSELoss()\n","num_epochs = 1\n","# Adam optimizerを使う\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"um1DG3-gksds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Reading RPM**"],"metadata":{"id":"J-Z7d7_okwrc"}},{"cell_type":"code","source":["# Reading RPM #\n","with open('/content/drive/MyDrive/PROJECT/RMSim_SeqX2Y/SeqX2Y-main/rpm_max.csv', 'r') as f:\n","    data = list(csv.reader(f, delimiter=\",\"))\n","\n","RPM = np.array(data)\n","RPM = np.float32(RPM)\n","test_RPM = RPM"],"metadata":{"id":"-q2wE27MkyIB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training**"],"metadata":{"id":"hq1WeDTjk3kd"}},{"cell_type":"code","source":["model.train() # モデルをトレーニングモードに設定\n","# トレーニングの循環部分\n","for epoch in range(num_epochs):\n","    for batch in dataloader:\n","        # Randomly choose RPM\n","        # patient = batch\n","        rpm = np.int(np.random.randint(0, 20, 1))\n","        # rpm = np.int(np.random.randint(1, 10, 1))\n","        # print(\"Patient index:\", patient,\"RPM index:\",rpm )\n","        test_rpm_ = test_RPM[rpm,:]\n","        test_x_rpm = test_RPM[rpm,:1]\n","        test_x_rpm = np.expand_dims(test_x_rpm,0)\n","        test_y_rpm = test_RPM[rpm,0:]\n","        test_y_rpm = np.expand_dims(test_y_rpm,0)\n","        test_x_rpm_tensor = torch.Tensor(test_x_rpm)\n","        test_y_rpm_tensor = torch.Tensor(test_y_rpm)\n","#___________________________________________________________________________________\n","\n","        bbatch=batch.unsqueeze(dim=0) # shape [1, 9 ,1 ,512 ,512]\n","        bbatch=bbatch.unsqueeze(dim=2) # shape [1, 9, 1, 1, 512, 512]\n","\n","        # (b, t, _, c, h, w)\n","        fake = torch.randn([1,3,3,3,512,512], dtype=torch.float)\n","\n","        # 勾配ゼロクリア\n","        optimizer.zero_grad()\n","        # 入力データを取得\n","        # inputs, targets = batch[0].to(device), batch[1].to(device)\n","        inputs = bbatch.float()\n","        # 順伝播 forward propagation\n","        outputs, DVF = model(fake, rpm_x=test_x_rpm_tensor, rpm_y=test_y_rpm_tensor, future_seq=9, hidden_state=None)\n","        # lossを計算\n","        loss = criterion(outputs, inputs)\n","        # 逆伝播 back propagation\n","        loss.backward()\n","        # パラメータの更新\n","        optimizer.step()\n","        # 現在のBatchのlossをprint\n","        print(f\"Epoch: {epoch+1}, Batch Loss: {loss.item()}\")\n","# モデルを保存する\n","torch.save(model, \"/content/drive/MyDrive/PROJECT/RMSim_SeqX2Y/My_Train_Model/model.pth\")"],"metadata":{"id":"wlwIS65Hk4Rv"},"execution_count":null,"outputs":[]}]}